# SMLT 


This is the repository of Serverless Machine Learning Training (SMLT) Framework

# SMLT Overview
<img align="center" src=https://github.com/Iam-ahsan/SMLT/blob/main/overview.png>
The above Figure summarizes the SMLT framework, describing the interactions between different modules during the training process. To start a training process, users provide the ML model, training data and training code which are uploaded to the object store, as well as their performance and budget requirements. First, the **Artifact manager** uploads the user-provided training code and data to cloud *Object Store* (1). Next, the **Task Scheduler starts the training process with initial (arbitrary) training resources (i.e., worker memory allocation and the number of workers) (2). As soon as the workers are deployed by the cloud provider, the **Data Iterator** in each worker  downloads the assigned training data and code (3). Then, in (4), the **Minibatch Buffer** loads the training data to memory from the local storage, after which the **Trainer** initiates the  training process (5). Upon the completion of a single iteration,  the generated model parameters are provided as input to the **Hierarchical Aggregator** (6), which is responsible for updating the model parameters using the mechanism described below (7). While the hierarchical aggregator updates the model parameters, the **Resource Manger** in the end client also reads the parameter metadata from the parameter store to check the health of each training worker (7b). Failure of any worker is detected through a missing key in the metadata after each iteration. Afterwards, the **Task Scheduler** collects the training performance (i.e., throughput) as well as the training status (i.e., current batch size, learning rate, epoch count, etc.) (8). The **Task Scheduler** also restarts any failed workers as necessary. The training performance and status are further utilized by the **Resource Manager** to optimize the training performance using Bayesian optimization (9). Finally, the updated training resources are shared with the **Task Scheduler** (10), and the process continues until a certain number of epochs or the desired accuracy is achieved.

# Hierarchical Aggregator 
<img aligh=center src=https://github.com/Iam-ahsan/SMLT/blob/main/Shard%20Aggregator.png>
Our mechanism performs the synchronization of gradients generated by all workers in a MapReduce-alike fashion among the workers themselves (Figure~\ref{fig:sharding_updated}). Specifically, after each iteration, our hierarchical synchronization mechanism takes the model gradients generated by each worker as input.  The \textit{shard generator} module,  residing in each of the $n$ workers, divides the input gradients into $m$ equal-sized shards \circled{1}. These shards are uploaded to the \textit{parameter store}  which acts as a communication intermediary between the stateless serverless workers \circled{2}. 
In {\framename}, each serverless worker also acts as a shard aggregator (thus, $n$ equals $m$, but they could also be different).
The sharded gradients are further downloaded by the \textit{shard aggregator} module  residing in each worker \circled{3}.  Each shard aggregator is responsible for aggregating a particular shard generated by all workers. For example, the 'shard aggregator 1' in above figure is responsible for aggregating the first shard from all workers to perform a mean operation. We call the resulting value as **aggregated shard**, which is again uploaded to the parameter store  by each shard aggregator \circled{4}. After this step, the parameter store contains the updated parameters of all workers in a sharded form. Finally, the **Global Aggregator** module residing in each worker downloads all the aggregated shards, and reconstructs the updated model for the next iteration \circled{5}.

---
**prerequisite**
- [AWS  Cli](https://aws.amazon.com/cli/)
- [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)
