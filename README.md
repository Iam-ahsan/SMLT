# SMLT
Serverless Machine Learning Training
This is our repository of Serverless Machine Learning Training Framework
**prerequisite**
# SMLT [![Overview](https://github.com/Iam-ahsan/SMLT/blob/main/overview.png)]
The above Figure summarizes the SMLT framework, describing the interactions between different modules during the training process. To start a training process, users provide the ML model, training data and training code which are uploaded to the object store, as well as their performance and budget requirements. First, the \emph{artifact manager} uploads the user-provided training code and data to cloud \emph{object store} \circledWhite{1}. Next, the \emph{task scheduler} starts the training process with initial (arbitrary) training resources (i.e., worker memory allocation and the number of workers) \circledWhite{2}. As soon as the workers are deployed by the cloud provider, the \emph{data iterator} in each worker  downloads the assigned training data and code \circledWhite{3}. Then, in \circledWhite{4}, the \emph{minibatch buffer} loads the training data to memory from the local storage, after which the \emph{trainer} initiates the  training process \circledWhite{5}. Upon the completion of a single iteration,  the generated model parameters are provided as input to the \emph{hierarchical aggregator} \circledWhite{6}, which is responsible for updating the model parameters using the mechanism described in Section \ref{sec:modelsync} \circledWhite{7a}. While the hierarchical aggregator updates the model parameters, the \emph{resource manger} in the end client also reads the parameter metadata from the parameter store to check the health of each training worker \circledWhite{7b}. Failure of any worker is detected through a missing key in the metadata after each iteration. Afterwards, the \emph{task scheduler} collects the training performance (i.e., throughput) as well as the training status (i.e., current batch size, learning rate, epoch count, etc.) \circledWhite{8}. The \emph{task scheduler} also restarts any failed workers as necessary. The training performance and status are further utilized by the \emph{resource manager} to optimize the training performance using Bayesian optimization \circledWhite{9}. Finally, the updated training resources are shared with the \emph{task scheduler} \circledWhite{10}, and the process continues until a certain number of epochs or the desired accuracy is achieved.
- [AWS  Cli](https://aws.amazon.com/cli/)
- [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)
